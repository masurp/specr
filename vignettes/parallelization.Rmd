---
title: "Using parallelization"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using parallelization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

A major improvement in version 0.3.0 of specr is that we can parallelize the computations, which can reduce fitting time. For this, specr uses functions from the package [furrr](https://furrr.futureverse.org/). I suggest to check out the website of the package for further information. 

Before we start to run some examples, bear in mind that using parallelization does not always mean that the computations are automatically faster. If the overall computation time is not very long, using parallelization may even lead to a longer fitting process as setting up several "workers" (essentially setting up procedures on several cores) produces a considerable "overhead". A true reduction in fitting time is thus only achieved when the data set is large, the number of specification are high, and the computed models are complex. I thus first simulate a comparatively large data set that allows to specify more than 1000 specifications. 

```{r, message = F, warning = F}
# Load packages
library(tidyverse)
library(specr)

# Data generating function
generate_data <- function(seed = 42, n = 1e5) {
  if (!is.na(seed)) set.seed(seed)
  dat <- tibble(
    x1 = rnorm(n),
    x2 = rnorm(n)+ 0.9*x1,
    x3 = rnorm(n)+ 0.9*x2,
    x4 = rnorm(n)+ 0.9*x3,
    y4 = rep(c(1, 0), times = n/2),
    y1 = rnorm(n) + x1*.1 * 0.9*y4,
    y2 = rnorm(n) + x1*.2,
    y3 = rnorm(n) + x1*.2 + -0.4*x2, 
    c1 = rnorm(n) + x1*.3,
    c2 = rnorm(n),
    c3 = rnorm(n) + 0.9*c1,
    c4 = rnorm(n),
    group = sample(c("a", "b", "c", "d", "e"), n, replace = TRUE)
  )
}

# Generate very large data set (n = 100,000, 9.2 MB on disk)
dat <- generate_data(9)
```

## Simple parallelization without custom functions

If we use standard model fitting function (e.g., "lm", "glm") that are included in the base package, parallelization is a simple as specifiying e.g., `workers = 4` to use 4 cores. If you don't specify anything, it will actually call a function `availableCores()` and use as many cores as your computer provides. 

```{r}
# Setup of specifications (number of specs = 1152)
specs <- setup(data = dat,            
               y = c("y1", "y2", "y3"),                    
               x = c("x1", "x2", "x3", "x4"),               
               model = c("lm"), 
               distinct(dat, group),
               controls = c("c1", "c2", "c3", "c4"))   

# Comparison
results_simple <- specr(specs, workers = 1)
results_parall <- specr(specs, workers = 4)
```

As we can see, the fitting time is reduced, but even with this comparatively large data set and more than 1,000 specifications, the reduction is still not that much. In sum, parallelization only makes sense if you have a truly large data set and thousands of specifications **or** the type of model you are estimating takes a really long time (e.g., a complex structural equation model, a negative binomial model, etc.). 


## Parallelization with custom functions from different packages. 

If we use a custom function, we need to make sure that this function is passed to the different workers. This can be done by specifying so called `furrr_options`. We need to pass objects from the global environment (and potentially also packages) as shown below.

```{r}
# Custom function
log_model <- function(formula, data) {
  glm(formula, data, family = binomial())
}

# Setup specs
specs <- setup(data = dat,            
               y = c("y4"),                    
               x = c("x1", "x2", "x3"),               
               model = c("log_model"), 
               distinct(dat, group),
               controls = c("c1", "c2"))   

# Creat furrr_options to be passed to specr()
opts <- furrr_options(
  globals = list(log_model = log_model)
)

# Run results
results_parall_2 <- specr(specs, 
                          workers = 4,
                          .options = opts)

# Summarize results
summary(results_parall_2)
```






